---
---
<!doctype html>
<html lang="en">
<head>
    <title>Ini Oguntola</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="custom.css">
    {% seo %}
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
    <script src="ja.js"></script>
</head>

<body>
    <span class="position-absolute trigger"><!-- hidden trigger to apply 'stuck' styles --></span>
    <nav class="navbar header-top fixed-top navbar-expand-md navbar-dark" id="site-header">
        <div class="d-flex flex-nowrap w-100">
            <a class="navbar-brand header-brand font-weight-light d-flex w-100" href="/">i/o</a>
            <button class="navbar-toggler d-md-none" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="true" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
        </div>
        <div class="collapse navbar-collapse" id="navbarNav">
            <div class="navbar-nav header-nav mx-auto">
                <a class="nav-item nav-link header-link active" href="/">Home<span class="sr-only">(current)</span></a>
                <a class="nav-item nav-link header-link" href="/about.html">About</a>
                <a class="nav-item nav-link header-link" href="/research.html">Research</a>
                <a class="nav-item nav-link header-link" href="/music.html">Music</a>
            </div>
        </div>
        <div class="d-flex w-100"> </div>
    </nav>

    <div class="image-bg" style="background-image:url('cmu_pano.jpg'); margin: -48px 0 0 0;">
        <div class="container-fluid" style="background-image: linear-gradient(to top, rgba(0,0,0,0), rgba(0,0,0,1));">
            <div class="row mb-5">
                <span class="my-4"></span>
            </div>

            <div class="row p-0">
                <div class="col">
                    <div class="container">
                    <div class="row mb-4 align-items-center">
                        <div class="col col-md-auto text-center text-md-left">
                            <img src="avatar.png" class="avatar rounded-circle" alt="avatar" width="200px" height="200px">
                        </div>

                        <div class="col-md mt-auto text-center text-md-left">
                            <h1 class="mt-3 mb-2" style="color:white; font-weight:300">Ini Oguntola</h1>
                            <p style="color:white; font-weight:300">
                                PhD Student in Machine Learning at Carnegie Mellon University
                            </p>

                            <ul class="list-inline">
                                <li class="list-inline-item"><a href="https://twitter.com/IniOguntola" class="fa fa-twitter" target="_blank"></a></li>
                                <li class="list-inline-item"><a href="https://github.com/ioguntol" class="fa fa-github" target="_blank"></a></li>
                                <li class="list-inline-item"><a href="https://www.youtube.com/channel/UCJfZOFJUnwkpCysC3I0omIw" class="fa fa-youtube-play" target="_blank"></a></li>
                                <li class="list-inline-item"><a href="https://soundcloud.com/iniog" class="fa fa-soundcloud" target="_blank"></a></li>
                            </ul>
                        </div>
                    </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <div class="container">
        <div class="row">
            <div class="col mt-5">
                <p>
                    I’m a 1st year Ph.D. student in Machine Learning at Carnegie Mellon University supervised by Katia Sycara.
                </p>
                <p>
                    Before that I was a student at MIT, where I graduated in 2019 with a B.S. in Computer Science, a B.S. in Mathematics, and a master’s degree in EECS with a concentration in AI. I’ve also spent time at Google and Microsoft Research.
                </p>
                <p>
                    I’m also an active musician and composer, working mostly in hip-hop, jazz, and R&B.
                </p>
                <p>
                    For the rest of todays lecture we will talk about a special class of statistics called sufficient
statistics. There are two ways to motivate sufficient statistics: the data reduction viewpoint
where we would like to discard non-informative pieces of the dataset (for storage or other
benefits), and the risk reduction viewpoint where we want to construct estimators that only
depend on meaningful variation in the data. We will focus on the former viewpoint today.
The goal in data reduction roughly is to find ways to reduce the size of a dataset without
throwing away important information. We need to fix ideas more concretely to make progress
on this abstract question.
We focus on parametric models and suppose we observe samples {X1, . . . , Xn} ∼ p(X; θ).
The typical statistical estimation problem is that we observe the samples as want to understand something about the unknown parameter θ. Again somewhat abstractly the goal of
data reduction is to find statistics T(X1, . . . , Xn) that contain all the information about the
unknown parameter θ.
For the rest of todays lecture we will talk about a special class of statistics called sufficient
statistics. There are two ways to motivate sufficient statistics: the data reduction viewpoint
where we would like to discard non-informative pieces of the dataset (for storage or other
benefits), and the risk reduction viewpoint where we want to construct estimators that only
depend on meaningful variation in the data. We will focus on the former viewpoint today.
The goal in data reduction roughly is to find ways to reduce the size of a dataset without
throwing away important information. We need to fix ideas more concretely to make progress
on this abstract question.
We focus on parametric models and suppose we observe samples {X1, . . . , Xn} ∼ p(X; θ).
The typical statistical estimation problem is that we observe the samples as want to understand something about the unknown parameter θ. Again somewhat abstractly the goal of
data reduction is to find statistics T(X1, . . . , Xn) that contain all the information about the
unknown parameter θ.For the rest of todays lecture we will talk about a special class of statistics called sufficient
statistics. There are two ways to motivate sufficient statistics: the data reduction viewpoint
where we would like to discard non-informative pieces of the dataset (for storage or other
benefits), and the risk reduction viewpoint where we want to construct estimators that only
depend on meaningful variation in the data. We will focus on the former viewpoint today.
The goal in data reduction roughly is to find ways to reduce the size of a dataset without
throwing away important information. We need to fix ideas more concretely to make progress
on this abstract question.
We focus on parametric models and suppose we observe samples {X1, . . . , Xn} ∼ p(X; θ).
The typical statistical estimation problem is that we observe the samples as want to understand something about the unknown parameter θ. Again somewhat abstractly the goal of
data reduction is to find statistics T(X1, . . . , Xn) that contain all the information about the
unknown parameter θ.
For the rest of todays lecture we will talk about a special class of statistics called sufficient
statistics. There are two ways to motivate sufficient statistics: the data reduction viewpoint
where we would like to discard non-informative pieces of the dataset (for storage or other
benefits), and the risk reduction viewpoint where we want to construct estimators that only
depend on meaningful variation in the data. We will focus on the former viewpoint today.
The goal in data reduction roughly is to find ways to reduce the size of a dataset without
throwing away important information. We need to fix ideas more concretely to make progress
on this abstract question.
We focus on parametric models and suppose we observe samples {X1, . . . , Xn} ∼ p(X; θ).
The typical statistical estimation problem is that we observe the samples as want to understand something about the unknown parameter θ. Again somewhat abstractly the goal of
data reduction is to find statistics T(X1, . . . , Xn) that contain all the information about the
unknown parameter θ.
For the rest of todays lecture we will talk about a special class of statistics called sufficient
statistics. There are two ways to motivate sufficient statistics: the data reduction viewpoint
where we would like to discard non-informative pieces of the dataset (for storage or other
benefits), and the risk reduction viewpoint where we want to construct estimators that only
depend on meaningful variation in the data. We will focus on the former viewpoint today.
The goal in data reduction roughly is to find ways to reduce the size of a dataset without
throwing away important information. We need to fix ideas more concretely to make progress
on this abstract question.
We focus on parametric models and suppose we observe samples {X1, . . . , Xn} ∼ p(X; θ).
The typical statistical estimation problem is that we observe the samples as want to understand something about the unknown parameter θ. Again somewhat abstractly the goal of
data reduction is to find statistics T(X1, . . . , Xn) that contain all the information about the
unknown parameter θ.For the rest of todays lecture we will talk about a special class of statistics called sufficient
statistics. There are two ways to motivate sufficient statistics: the data reduction viewpoint
where we would like to discard non-informative pieces of the dataset (for storage or other
benefits), and the risk reduction viewpoint where we want to construct estimators that only
depend on meaningful variation in the data. We will focus on the former viewpoint today.
The goal in data reduction roughly is to find ways to reduce the size of a dataset without
throwing away important information. We need to fix ideas more concretely to make progress
on this abstract question.
We focus on parametric models and suppose we observe samples {X1, . . . , Xn} ∼ p(X; θ).
The typical statistical estimation problem is that we observe the samples as want to understand something about the unknown parameter θ. Again somewhat abstractly the goal of
data reduction is to find statistics T(X1, . . . , Xn) that contain all the information about the
unknown parameter θ.
                </p>
            </div>
        </div>         
    </div>
</body>
</html>



